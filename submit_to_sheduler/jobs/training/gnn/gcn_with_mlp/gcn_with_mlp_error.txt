DeprecationWarning: 'source deactivate' is deprecated. Use 'conda deactivate'.
Global seed set to 5
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_d6ec57 | 174 K 
------------------------------------------------
174 K     Trainable params
0         Non-trainable params
174 K     Total params
0.696     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 17408. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 15232. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 10:31:52,695][0m Trial 75 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_465cbb | 381 K 
------------------------------------------------
381 K     Trainable params
0         Non-trainable params
381 K     Total params
1.526     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 10:34:00,510][0m Trial 76 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_928e0e | 120 K 
------------------------------------------------
120 K     Trainable params
0         Non-trainable params
120 K     Total params
0.483     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 10:36:19,440][0m Trial 77 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_e55b92 | 1.0 M 
------------------------------------------------
1.0 M     Trainable params
0         Non-trainable params
1.0 M     Total params
4.022     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 10:38:30,452][0m Trial 78 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_337526 | 275 K 
------------------------------------------------
275 K     Trainable params
0         Non-trainable params
275 K     Total params
1.104     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 10:38:30,593][0m Trial 79 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_338720 | 203 K 
------------------------------------------------
203 K     Trainable params
0         Non-trainable params
203 K     Total params
0.815     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 10:40:44,026][0m Trial 80 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_8312a2 | 118 K 
------------------------------------------------
118 K     Trainable params
0         Non-trainable params
118 K     Total params
0.473     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 10:40:44,164][0m Trial 81 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_832679 | 163 K 
------------------------------------------------
163 K     Trainable params
0         Non-trainable params
163 K     Total params
0.654     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 10:43:14,138][0m Trial 82 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_dc8b75 | 80.0 K
------------------------------------------------
80.0 K    Trainable params
0         Non-trainable params
80.0 K    Total params
0.320     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 10:43:14,272][0m Trial 83 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_dc9f54 | 371 K 
------------------------------------------------
371 K     Trainable params
0         Non-trainable params
371 K     Total params
1.486     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 10:45:41,918][0m Trial 84 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_34a240 | 1.5 M 
------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
5.835     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 12:49:26,501][0m Trial 85 finished with value: 0.049153830856084824 and parameters: {'num_of_gcn_layers': 1, 'gcn_output_dim_0': 69, 'num_of_mlp_layers': 9, 'mlp_output_dim_0': 301, 'mlp_output_dim_1': 7, 'mlp_output_dim_2': 17, 'mlp_output_dim_3': 169, 'mlp_output_dim_4': 177, 'mlp_output_dim_5': 11, 'mlp_output_dim_6': 33, 'mlp_output_dim_7': 57, 'mlp_output_dim_8': 72}. Best is trial 19 with value: 0.041568487882614136.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_7e0814 | 1.5 M 
------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.051     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 12:51:52,292][0m Trial 86 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_d4ed15 | 1.7 M 
------------------------------------------------
1.7 M     Trainable params
0         Non-trainable params
1.7 M     Total params
6.615     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 14:49:05,767][0m Trial 87 finished with value: 0.04724420979619026 and parameters: {'num_of_gcn_layers': 1, 'gcn_output_dim_0': 61, 'num_of_mlp_layers': 8, 'mlp_output_dim_0': 391, 'mlp_output_dim_1': 7, 'mlp_output_dim_2': 14, 'mlp_output_dim_3': 60, 'mlp_output_dim_4': 331, 'mlp_output_dim_5': 14, 'mlp_output_dim_6': 11, 'mlp_output_dim_7': 56}. Best is trial 19 with value: 0.041568487882614136.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_3535ae | 1.7 M 
------------------------------------------------
1.7 M     Trainable params
0         Non-trainable params
1.7 M     Total params
6.670     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 14:51:28,409][0m Trial 88 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_8a383c | 1.0 M 
------------------------------------------------
1.0 M     Trainable params
0         Non-trainable params
1.0 M     Total params
4.066     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 14:51:28,531][0m Trial 89 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_8a49e6 | 196 K 
------------------------------------------------
196 K     Trainable params
0         Non-trainable params
196 K     Total params
0.786     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 14:51:28,641][0m Trial 90 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_8a5c1e | 1.7 M 
------------------------------------------------
1.7 M     Trainable params
0         Non-trainable params
1.7 M     Total params
6.873     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 17:04:08,347][0m Trial 91 finished with value: 0.0469057559967041 and parameters: {'num_of_gcn_layers': 1, 'gcn_output_dim_0': 89, 'num_of_mlp_layers': 9, 'mlp_output_dim_0': 281, 'mlp_output_dim_1': 15, 'mlp_output_dim_2': 9, 'mlp_output_dim_3': 27, 'mlp_output_dim_4': 241, 'mlp_output_dim_5': 7, 'mlp_output_dim_6': 19, 'mlp_output_dim_7': 39, 'mlp_output_dim_8': 37}. Best is trial 19 with value: 0.041568487882614136.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_12b97c | 1.1 M 
------------------------------------------------
1.1 M     Trainable params
0         Non-trainable params
1.1 M     Total params
4.246     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 17:04:08,514][0m Trial 92 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_12d150 | 2.3 M 
------------------------------------------------
2.3 M     Trainable params
0         Non-trainable params
2.3 M     Total params
9.064     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 17:04:08,652][0m Trial 93 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_12e5de | 2.1 M 
------------------------------------------------
2.1 M     Trainable params
0         Non-trainable params
2.1 M     Total params
8.514     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 17:17:17,985][0m Trial 94 pruned. Trial was pruned at epoch 9.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_e9616d | 136 K 
------------------------------------------------
136 K     Trainable params
0         Non-trainable params
136 K     Total params
0.545     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 17:17:18,139][0m Trial 95 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_e978a1 | 1.6 M 
------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.441     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 19:27:58,563][0m Trial 96 finished with value: 0.044320326298475266 and parameters: {'num_of_gcn_layers': 1, 'gcn_output_dim_0': 86, 'num_of_mlp_layers': 8, 'mlp_output_dim_0': 271, 'mlp_output_dim_1': 78, 'mlp_output_dim_2': 16, 'mlp_output_dim_3': 31, 'mlp_output_dim_4': 12, 'mlp_output_dim_5': 7, 'mlp_output_dim_6': 9, 'mlp_output_dim_7': 36}. Best is trial 19 with value: 0.041568487882614136.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_2abd0b | 1.0 M 
------------------------------------------------
1.0 M     Trainable params
0         Non-trainable params
1.0 M     Total params
4.084     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 19:27:58,734][0m Trial 97 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_2ad488 | 2.1 M 
------------------------------------------------
2.1 M     Trainable params
0         Non-trainable params
2.1 M     Total params
8.402     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 19:27:58,877][0m Trial 98 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e8f7a2ca-6120-9306-30b8-58b417c5eed6]

  | Name      | Type              | Params
------------------------------------------------
0 | loss_fn   | MSELoss           | 0     
1 | criterion | MSELoss           | 0     
2 | model     | Sequential_2ae9a4 | 2.1 M 
------------------------------------------------
2.1 M     Trainable params
0         Non-trainable params
2.1 M     Total params
8.220     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 19:27:59,015][0m Trial 99 pruned. Trial was pruned at epoch 0.[0m
DeprecationWarning: 'source deactivate' is deprecated. Use 'conda deactivate'.
