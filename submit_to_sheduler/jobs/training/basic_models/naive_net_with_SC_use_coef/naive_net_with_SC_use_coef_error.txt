DeprecationWarning: 'source deactivate' is deprecated. Use 'conda deactivate'.
Global seed set to 5
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 28.6 K
3 | layers              | Sequential       | 13.7 K
---------------------------------------------------------
42.4 K    Trainable params
0         Non-trainable params
42.4 K    Total params
0.169     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-09 21:19:04,124][0m Trial 50 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 258 K 
3 | layers              | Sequential       | 17.8 K
---------------------------------------------------------
276 K     Trainable params
0         Non-trainable params
276 K     Total params
1.105     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-10 00:23:44,121][0m Trial 51 finished with value: 0.07070215791463852 and parameters: {'num_of_extract_SC_feat_layers': 2, 'extract_SC_feat_output_dim_l0': 113, 'extract_SC_feat_output_dim_l1': 5, 'num_of_naive_net_layers': 5, 'naive_net_output_dim_l0': 11, 'naive_net_output_dim_l1': 94, 'naive_net_output_dim_l2': 139, 'naive_net_output_dim_l3': 13, 'naive_net_output_dim_l4': 50}. Best is trial 4 with value: 0.06918008625507355.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 304 K 
3 | layers              | Sequential       | 26.4 K
---------------------------------------------------------
330 K     Trainable params
0         Non-trainable params
330 K     Total params
1.322     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:23:44,268][0m Trial 52 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 231 K 
3 | layers              | Sequential       | 14.9 K
---------------------------------------------------------
245 K     Trainable params
0         Non-trainable params
245 K     Total params
0.984     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:23:44,407][0m Trial 53 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 165 K 
3 | layers              | Sequential       | 21.2 K
---------------------------------------------------------
187 K     Trainable params
0         Non-trainable params
187 K     Total params
0.748     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:23:44,544][0m Trial 54 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 452 K 
3 | layers              | Sequential       | 3.1 K 
---------------------------------------------------------
455 K     Trainable params
0         Non-trainable params
455 K     Total params
1.823     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:23:44,686][0m Trial 55 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 114 K 
3 | layers              | Sequential       | 4.4 K 
---------------------------------------------------------
119 K     Trainable params
0         Non-trainable params
119 K     Total params
0.477     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:23:44,822][0m Trial 56 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 339 K 
3 | layers              | Sequential       | 5.9 K 
---------------------------------------------------------
345 K     Trainable params
0         Non-trainable params
345 K     Total params
1.383     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:23:44,953][0m Trial 57 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 23.1 K
3 | layers              | Sequential       | 50.9 K
---------------------------------------------------------
74.0 K    Trainable params
0         Non-trainable params
74.0 K    Total params
0.296     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:23:45,096][0m Trial 58 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 20.7 K
3 | layers              | Sequential       | 34.6 K
---------------------------------------------------------
55.3 K    Trainable params
0         Non-trainable params
55.3 K    Total params
0.221     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:23:45,238][0m Trial 59 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 32.4 K
3 | layers              | Sequential       | 10.6 K
---------------------------------------------------------
43.0 K    Trainable params
0         Non-trainable params
43.0 K    Total params
0.172     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-10 00:27:26,094][0m Trial 60 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 249 K 
3 | layers              | Sequential       | 8.5 K 
---------------------------------------------------------
257 K     Trainable params
0         Non-trainable params
257 K     Total params
1.030     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:27:26,239][0m Trial 61 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 219 K 
3 | layers              | Sequential       | 11.3 K
---------------------------------------------------------
230 K     Trainable params
0         Non-trainable params
230 K     Total params
0.923     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-10 00:31:10,954][0m Trial 62 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 287 K 
3 | layers              | Sequential       | 12.9 K
---------------------------------------------------------
300 K     Trainable params
0         Non-trainable params
300 K     Total params
1.203     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:31:11,102][0m Trial 63 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 567 K 
3 | layers              | Sequential       | 22.6 K
---------------------------------------------------------
590 K     Trainable params
0         Non-trainable params
590 K     Total params
2.362     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:31:11,251][0m Trial 64 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 312 K 
3 | layers              | Sequential       | 6.7 K 
---------------------------------------------------------
319 K     Trainable params
0         Non-trainable params
319 K     Total params
1.277     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:31:11,389][0m Trial 65 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 178 K 
3 | layers              | Sequential       | 496   
---------------------------------------------------------
178 K     Trainable params
0         Non-trainable params
178 K     Total params
0.715     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-10 00:34:58,098][0m Trial 66 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 375 K 
3 | layers              | Sequential       | 1.4 K 
---------------------------------------------------------
376 K     Trainable params
0         Non-trainable params
376 K     Total params
1.507     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-10 00:38:39,428][0m Trial 67 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 11.4 K
3 | layers              | Sequential       | 9.0 K 
---------------------------------------------------------
20.3 K    Trainable params
0         Non-trainable params
20.3 K    Total params
0.081     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:38:39,577][0m Trial 68 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 75.3 K
3 | layers              | Sequential       | 7.7 K 
---------------------------------------------------------
83.0 K    Trainable params
0         Non-trainable params
83.0 K    Total params
0.332     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-10 00:42:17,600][0m Trial 69 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 48.7 K
3 | layers              | Sequential       | 5.9 K 
---------------------------------------------------------
54.5 K    Trainable params
0         Non-trainable params
54.5 K    Total params
0.218     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:42:17,742][0m Trial 70 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 16.0 K
3 | layers              | Sequential       | 7.3 K 
---------------------------------------------------------
23.3 K    Trainable params
0         Non-trainable params
23.3 K    Total params
0.093     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:42:17,880][0m Trial 71 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 11.5 K
3 | layers              | Sequential       | 7.6 K 
---------------------------------------------------------
19.1 K    Trainable params
0         Non-trainable params
19.1 K    Total params
0.076     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:42:18,020][0m Trial 72 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 25.8 K
3 | layers              | Sequential       | 42.6 K
---------------------------------------------------------
68.4 K    Trainable params
0         Non-trainable params
68.4 K    Total params
0.274     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-10 00:46:08,456][0m Trial 73 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 13.7 K
3 | layers              | Sequential       | 4.1 K 
---------------------------------------------------------
17.8 K    Trainable params
0         Non-trainable params
17.8 K    Total params
0.071     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:46:08,603][0m Trial 74 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 483 K 
3 | layers              | Sequential       | 13.5 K
---------------------------------------------------------
497 K     Trainable params
0         Non-trainable params
497 K     Total params
1.989     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:46:08,751][0m Trial 75 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 25.4 K
3 | layers              | Sequential       | 43.5 K
---------------------------------------------------------
68.9 K    Trainable params
0         Non-trainable params
68.9 K    Total params
0.276     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:46:08,891][0m Trial 76 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 148 K 
3 | layers              | Sequential       | 26.4 K
---------------------------------------------------------
175 K     Trainable params
0         Non-trainable params
175 K     Total params
0.701     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:46:09,027][0m Trial 77 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 18.2 K
3 | layers              | Sequential       | 7.8 K 
---------------------------------------------------------
26.0 K    Trainable params
0         Non-trainable params
26.0 K    Total params
0.104     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:46:09,165][0m Trial 78 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 9.2 K 
3 | layers              | Sequential       | 1.7 K 
---------------------------------------------------------
11.0 K    Trainable params
0         Non-trainable params
11.0 K    Total params
0.044     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:46:09,303][0m Trial 79 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 216 K 
3 | layers              | Sequential       | 32.2 K
---------------------------------------------------------
248 K     Trainable params
0         Non-trainable params
248 K     Total params
0.996     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:46:09,435][0m Trial 80 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 390 K 
3 | layers              | Sequential       | 12.3 K
---------------------------------------------------------
403 K     Trainable params
0         Non-trainable params
403 K     Total params
1.613     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-10 00:49:56,141][0m Trial 81 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 240 K 
3 | layers              | Sequential       | 13.0 K
---------------------------------------------------------
253 K     Trainable params
0         Non-trainable params
253 K     Total params
1.013     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:49:56,289][0m Trial 82 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 290 K 
3 | layers              | Sequential       | 2.7 K 
---------------------------------------------------------
293 K     Trainable params
0         Non-trainable params
293 K     Total params
1.174     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:49:56,434][0m Trial 83 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 256 K 
3 | layers              | Sequential       | 4.9 K 
---------------------------------------------------------
260 K     Trainable params
0         Non-trainable params
260 K     Total params
1.044     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:49:56,577][0m Trial 84 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 182 K 
3 | layers              | Sequential       | 15.9 K
---------------------------------------------------------
198 K     Trainable params
0         Non-trainable params
198 K     Total params
0.795     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-10 00:53:44,685][0m Trial 85 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 336 K 
3 | layers              | Sequential       | 17.6 K
---------------------------------------------------------
354 K     Trainable params
0         Non-trainable params
354 K     Total params
1.417     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-10 00:57:40,712][0m Trial 86 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 204 K 
3 | layers              | Sequential       | 8.1 K 
---------------------------------------------------------
212 K     Trainable params
0         Non-trainable params
212 K     Total params
0.850     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:57:40,856][0m Trial 87 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 41.1 K
3 | layers              | Sequential       | 30.2 K
---------------------------------------------------------
71.2 K    Trainable params
0         Non-trainable params
71.2 K    Total params
0.285     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:57:40,996][0m Trial 88 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 12.0 K
3 | layers              | Sequential       | 21.9 K
---------------------------------------------------------
33.9 K    Trainable params
0         Non-trainable params
33.9 K    Total params
0.136     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:57:41,136][0m Trial 89 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 503 K 
3 | layers              | Sequential       | 30.4 K
---------------------------------------------------------
534 K     Trainable params
0         Non-trainable params
534 K     Total params
2.136     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:57:41,293][0m Trial 90 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 332 K 
3 | layers              | Sequential       | 14.7 K
---------------------------------------------------------
346 K     Trainable params
0         Non-trainable params
346 K     Total params
1.388     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 00:57:41,441][0m Trial 91 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 409 K 
3 | layers              | Sequential       | 16.2 K
---------------------------------------------------------
425 K     Trainable params
0         Non-trainable params
425 K     Total params
1.703     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-10 04:10:16,930][0m Trial 92 finished with value: 0.07198730111122131 and parameters: {'num_of_extract_SC_feat_layers': 4, 'extract_SC_feat_output_dim_l0': 179, 'extract_SC_feat_output_dim_l1': 4, 'extract_SC_feat_output_dim_l2': 10, 'extract_SC_feat_output_dim_l3': 37, 'num_of_naive_net_layers': 5, 'naive_net_output_dim_l0': 30, 'naive_net_output_dim_l1': 86, 'naive_net_output_dim_l2': 82, 'naive_net_output_dim_l3': 29, 'naive_net_output_dim_l4': 60}. Best is trial 4 with value: 0.06918008625507355.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 277 K 
3 | layers              | Sequential       | 16.2 K
---------------------------------------------------------
293 K     Trainable params
0         Non-trainable params
293 K     Total params
1.174     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-10 04:14:12,033][0m Trial 93 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 414 K 
3 | layers              | Sequential       | 13.2 K
---------------------------------------------------------
427 K     Trainable params
0         Non-trainable params
427 K     Total params
1.712     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 04:14:12,196][0m Trial 94 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 363 K 
3 | layers              | Sequential       | 26.9 K
---------------------------------------------------------
390 K     Trainable params
0         Non-trainable params
390 K     Total params
1.560     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 04:14:12,352][0m Trial 95 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 310 K 
3 | layers              | Sequential       | 6.6 K 
---------------------------------------------------------
317 K     Trainable params
0         Non-trainable params
317 K     Total params
1.270     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-10 04:18:02,080][0m Trial 96 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 175 K 
3 | layers              | Sequential       | 8.4 K 
---------------------------------------------------------
183 K     Trainable params
0         Non-trainable params
183 K     Total params
0.735     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 04:18:02,235][0m Trial 97 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 121 K 
3 | layers              | Sequential       | 1.9 K 
---------------------------------------------------------
123 K     Trainable params
0         Non-trainable params
123 K     Total params
0.493     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 04:18:02,376][0m Trial 98 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-5321c688-062b-01cb-cfa7-a24dde08e3ea]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 16.0 K
3 | layers              | Sequential       | 14.9 K
---------------------------------------------------------
31.0 K    Trainable params
0         Non-trainable params
31.0 K    Total params
0.124     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-10 04:18:02,517][0m Trial 99 pruned. Trial was pruned at epoch 0.[0m
DeprecationWarning: 'source deactivate' is deprecated. Use 'conda deactivate'.
