DeprecationWarning: 'source deactivate' is deprecated. Use 'conda deactivate'.
Global seed set to 5
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 55.9 K
-----------------------------------------
55.9 K    Trainable params
0         Non-trainable params
55.9 K    Total params
0.223     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 20:24:16,853][0m Trial 50 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 26.7 K
-----------------------------------------
26.7 K    Trainable params
0         Non-trainable params
26.7 K    Total params
0.107     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 20:24:16,917][0m Trial 51 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 43.8 K
-----------------------------------------
43.8 K    Trainable params
0         Non-trainable params
43.8 K    Total params
0.175     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 20:24:16,975][0m Trial 52 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 49.2 K
-----------------------------------------
49.2 K    Trainable params
0         Non-trainable params
49.2 K    Total params
0.197     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 20:24:17,037][0m Trial 53 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 55.7 K
-----------------------------------------
55.7 K    Trainable params
0         Non-trainable params
55.7 K    Total params
0.223     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 20:24:17,100][0m Trial 54 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 15.1 K
-----------------------------------------
15.1 K    Trainable params
0         Non-trainable params
15.1 K    Total params
0.060     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-26 20:24:52,848][0m Trial 55 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 52.9 K
-----------------------------------------
52.9 K    Trainable params
0         Non-trainable params
52.9 K    Total params
0.212     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-26 21:01:29,737][0m Trial 56 finished with value: 0.01848914846777916 and parameters: {'num_of_naive_net_layers': 5, 'naive_net_output_dim_l0': 46, 'naive_net_output_dim_l1': 218, 'naive_net_output_dim_l2': 85, 'naive_net_output_dim_l3': 149, 'naive_net_output_dim_l4': 5}. Best is trial 11 with value: 0.014216277748346329.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 31.4 K
-----------------------------------------
31.4 K    Trainable params
0         Non-trainable params
31.4 K    Total params
0.126     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 21:01:29,812][0m Trial 57 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 5.6 K 
-----------------------------------------
5.6 K     Trainable params
0         Non-trainable params
5.6 K     Total params
0.022     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 21:01:29,868][0m Trial 58 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 28.5 K
-----------------------------------------
28.5 K    Trainable params
0         Non-trainable params
28.5 K    Total params
0.114     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 21:01:29,943][0m Trial 59 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 116 K 
-----------------------------------------
116 K     Trainable params
0         Non-trainable params
116 K     Total params
0.467     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 21:01:29,995][0m Trial 60 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 53.2 K
-----------------------------------------
53.2 K    Trainable params
0         Non-trainable params
53.2 K    Total params
0.213     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 21:01:30,058][0m Trial 61 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 52.6 K
-----------------------------------------
52.6 K    Trainable params
0         Non-trainable params
52.6 K    Total params
0.210     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 21:01:30,120][0m Trial 62 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 55.1 K
-----------------------------------------
55.1 K    Trainable params
0         Non-trainable params
55.1 K    Total params
0.221     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 21:01:30,183][0m Trial 63 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 51.3 K
-----------------------------------------
51.3 K    Trainable params
0         Non-trainable params
51.3 K    Total params
0.205     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 21:01:30,246][0m Trial 64 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 26.6 K
-----------------------------------------
26.6 K    Trainable params
0         Non-trainable params
26.6 K    Total params
0.106     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-26 21:41:34,093][0m Trial 65 finished with value: 0.01657760888338089 and parameters: {'num_of_naive_net_layers': 8, 'naive_net_output_dim_l0': 34, 'naive_net_output_dim_l1': 222, 'naive_net_output_dim_l2': 30, 'naive_net_output_dim_l3': 5, 'naive_net_output_dim_l4': 10, 'naive_net_output_dim_l5': 4, 'naive_net_output_dim_l6': 37, 'naive_net_output_dim_l7': 92}. Best is trial 11 with value: 0.014216277748346329.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 15.7 K
-----------------------------------------
15.7 K    Trainable params
0         Non-trainable params
15.7 K    Total params
0.063     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 21:41:34,172][0m Trial 66 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 21.4 K
-----------------------------------------
21.4 K    Trainable params
0         Non-trainable params
21.4 K    Total params
0.086     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 21:41:34,243][0m Trial 67 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 38.3 K
-----------------------------------------
38.3 K    Trainable params
0         Non-trainable params
38.3 K    Total params
0.153     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 21:41:34,317][0m Trial 68 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 47.7 K
-----------------------------------------
47.7 K    Trainable params
0         Non-trainable params
47.7 K    Total params
0.191     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 21:41:34,387][0m Trial 69 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 8.6 K 
-----------------------------------------
8.6 K     Trainable params
0         Non-trainable params
8.6 K     Total params
0.034     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 21:41:34,470][0m Trial 70 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 34.4 K
-----------------------------------------
34.4 K    Trainable params
0         Non-trainable params
34.4 K    Total params
0.137     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 21:41:34,527][0m Trial 71 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 38.7 K
-----------------------------------------
38.7 K    Trainable params
0         Non-trainable params
38.7 K    Total params
0.155     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 21:41:34,595][0m Trial 72 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 32.1 K
-----------------------------------------
32.1 K    Trainable params
0         Non-trainable params
32.1 K    Total params
0.129     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 21:41:34,672][0m Trial 73 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 46.1 K
-----------------------------------------
46.1 K    Trainable params
0         Non-trainable params
46.1 K    Total params
0.184     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-26 22:23:35,844][0m Trial 74 finished with value: 0.02062038704752922 and parameters: {'num_of_naive_net_layers': 8, 'naive_net_output_dim_l0': 24, 'naive_net_output_dim_l1': 189, 'naive_net_output_dim_l2': 158, 'naive_net_output_dim_l3': 15, 'naive_net_output_dim_l4': 10, 'naive_net_output_dim_l5': 16, 'naive_net_output_dim_l6': 58, 'naive_net_output_dim_l7': 27}. Best is trial 11 with value: 0.014216277748346329.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 27.9 K
-----------------------------------------
27.9 K    Trainable params
0         Non-trainable params
27.9 K    Total params
0.112     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:23:35,912][0m Trial 75 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 19.4 K
-----------------------------------------
19.4 K    Trainable params
0         Non-trainable params
19.4 K    Total params
0.078     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:23:35,975][0m Trial 76 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 9.4 K 
-----------------------------------------
9.4 K     Trainable params
0         Non-trainable params
9.4 K     Total params
0.038     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:23:36,042][0m Trial 77 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 29.8 K
-----------------------------------------
29.8 K    Trainable params
0         Non-trainable params
29.8 K    Total params
0.119     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:23:36,099][0m Trial 78 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 19.8 K
-----------------------------------------
19.8 K    Trainable params
0         Non-trainable params
19.8 K    Total params
0.079     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:23:36,165][0m Trial 79 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 36.3 K
-----------------------------------------
36.3 K    Trainable params
0         Non-trainable params
36.3 K    Total params
0.145     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:23:36,237][0m Trial 80 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 16.6 K
-----------------------------------------
16.6 K    Trainable params
0         Non-trainable params
16.6 K    Total params
0.066     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:23:36,305][0m Trial 81 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 10.6 K
-----------------------------------------
10.6 K    Trainable params
0         Non-trainable params
10.6 K    Total params
0.042     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:23:36,383][0m Trial 82 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 12.3 K
-----------------------------------------
12.3 K    Trainable params
0         Non-trainable params
12.3 K    Total params
0.049     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-26 22:56:42,357][0m Trial 83 finished with value: 0.015729548409581184 and parameters: {'num_of_naive_net_layers': 6, 'naive_net_output_dim_l0': 30, 'naive_net_output_dim_l1': 32, 'naive_net_output_dim_l2': 99, 'naive_net_output_dim_l3': 7, 'naive_net_output_dim_l4': 8, 'naive_net_output_dim_l5': 56}. Best is trial 11 with value: 0.014216277748346329.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 11.6 K
-----------------------------------------
11.6 K    Trainable params
0         Non-trainable params
11.6 K    Total params
0.046     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:56:42,425][0m Trial 84 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 14.4 K
-----------------------------------------
14.4 K    Trainable params
0         Non-trainable params
14.4 K    Total params
0.058     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:56:42,487][0m Trial 85 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 9.7 K 
-----------------------------------------
9.7 K     Trainable params
0         Non-trainable params
9.7 K     Total params
0.039     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:56:42,545][0m Trial 86 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 8.1 K 
-----------------------------------------
8.1 K     Trainable params
0         Non-trainable params
8.1 K     Total params
0.032     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:56:42,618][0m Trial 87 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 62.4 K
-----------------------------------------
62.4 K    Trainable params
0         Non-trainable params
62.4 K    Total params
0.250     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:56:42,681][0m Trial 88 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 15.8 K
-----------------------------------------
15.8 K    Trainable params
0         Non-trainable params
15.8 K    Total params
0.063     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:56:42,743][0m Trial 89 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 84.6 K
-----------------------------------------
84.6 K    Trainable params
0         Non-trainable params
84.6 K    Total params
0.338     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-26 22:57:14,465][0m Trial 90 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 11.2 K
-----------------------------------------
11.2 K    Trainable params
0         Non-trainable params
11.2 K    Total params
0.045     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:57:14,536][0m Trial 91 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 11.2 K
-----------------------------------------
11.2 K    Trainable params
0         Non-trainable params
11.2 K    Total params
0.045     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:57:14,605][0m Trial 92 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 18.9 K
-----------------------------------------
18.9 K    Trainable params
0         Non-trainable params
18.9 K    Total params
0.075     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-26 22:57:56,594][0m Trial 93 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 39.4 K
-----------------------------------------
39.4 K    Trainable params
0         Non-trainable params
39.4 K    Total params
0.158     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:57:56,678][0m Trial 94 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 54.4 K
-----------------------------------------
54.4 K    Trainable params
0         Non-trainable params
54.4 K    Total params
0.218     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:57:56,751][0m Trial 95 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 12.7 K
-----------------------------------------
12.7 K    Trainable params
0         Non-trainable params
12.7 K    Total params
0.051     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:57:56,816][0m Trial 96 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 25.8 K
-----------------------------------------
25.8 K    Trainable params
0         Non-trainable params
25.8 K    Total params
0.103     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-26 22:58:36,665][0m Trial 97 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 54.0 K
-----------------------------------------
54.0 K    Trainable params
0         Non-trainable params
54.0 K    Total params
0.216     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:58:36,740][0m Trial 98 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 57.2 K
-----------------------------------------
57.2 K    Trainable params
0         Non-trainable params
57.2 K    Total params
0.229     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 22:58:36,795][0m Trial 99 pruned. Trial was pruned at epoch 0.[0m
DeprecationWarning: 'source deactivate' is deprecated. Use 'conda deactivate'.
