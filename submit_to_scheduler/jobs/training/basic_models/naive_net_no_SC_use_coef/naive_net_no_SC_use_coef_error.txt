DeprecationWarning: 'source deactivate' is deprecated. Use 'conda deactivate'.
Global seed set to 5
[32m[I 2022-08-07 19:06:27,076][0m A new study created in memory with name: no-name-1fcd90ae-fc48-4c96-9db1-706550b4360c[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 6.0 K 
-----------------------------------------
6.0 K     Trainable params
0         Non-trainable params
6.0 K     Total params
0.024     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-07 19:31:29,521][0m Trial 0 finished with value: 0.07329858839511871 and parameters: {'num_of_naive_net_layers': 3, 'naive_net_output_dim_l0': 147, 'naive_net_output_dim_l1': 9, 'naive_net_output_dim_l2': 181}. Best is trial 0 with value: 0.07329858839511871.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 9.1 K 
-----------------------------------------
9.1 K     Trainable params
0         Non-trainable params
9.1 K     Total params
0.037     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-07 20:02:18,648][0m Trial 1 finished with value: 0.07375141233205795 and parameters: {'num_of_naive_net_layers': 5, 'naive_net_output_dim_l0': 48, 'naive_net_output_dim_l1': 94, 'naive_net_output_dim_l2': 32, 'naive_net_output_dim_l3': 13, 'naive_net_output_dim_l4': 8}. Best is trial 0 with value: 0.07329858839511871.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 1.3 K 
-----------------------------------------
1.3 K     Trainable params
0         Non-trainable params
1.3 K     Total params
0.005     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-07 20:24:13,573][0m Trial 2 finished with value: 0.07335316389799118 and parameters: {'num_of_naive_net_layers': 1, 'naive_net_output_dim_l0': 83}. Best is trial 0 with value: 0.07329858839511871.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 4.0 K 
-----------------------------------------
4.0 K     Trainable params
0         Non-trainable params
4.0 K     Total params
0.016     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-07 20:54:29,136][0m Trial 3 finished with value: 0.0785781666636467 and parameters: {'num_of_naive_net_layers': 5, 'naive_net_output_dim_l0': 7, 'naive_net_output_dim_l1': 153, 'naive_net_output_dim_l2': 11, 'naive_net_output_dim_l3': 21, 'naive_net_output_dim_l4': 12}. Best is trial 0 with value: 0.07329858839511871.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 3.8 K 
-----------------------------------------
3.8 K     Trainable params
0         Non-trainable params
3.8 K     Total params
0.015     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-07 21:31:10,110][0m Trial 4 finished with value: 0.06982595473527908 and parameters: {'num_of_naive_net_layers': 7, 'naive_net_output_dim_l0': 42, 'naive_net_output_dim_l1': 46, 'naive_net_output_dim_l2': 11, 'naive_net_output_dim_l3': 12, 'naive_net_output_dim_l4': 10, 'naive_net_output_dim_l5': 14, 'naive_net_output_dim_l6': 7}. Best is trial 4 with value: 0.06982595473527908.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 51.7 K
-----------------------------------------
51.7 K    Trainable params
0         Non-trainable params
51.7 K    Total params
0.207     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 21:31:10,156][0m Trial 5 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 135   
-----------------------------------------
135       Trainable params
0         Non-trainable params
135       Total params
0.001     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 21:31:10,200][0m Trial 6 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 19.0 K
-----------------------------------------
19.0 K    Trainable params
0         Non-trainable params
19.0 K    Total params
0.076     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 21:31:10,242][0m Trial 7 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 46.2 K
-----------------------------------------
46.2 K    Trainable params
0         Non-trainable params
46.2 K    Total params
0.185     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-07 22:01:22,362][0m Trial 8 finished with value: 0.06962297111749649 and parameters: {'num_of_naive_net_layers': 3, 'naive_net_output_dim_l0': 110, 'naive_net_output_dim_l1': 147, 'naive_net_output_dim_l2': 184}. Best is trial 8 with value: 0.06962297111749649.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 419   
-----------------------------------------
419       Trainable params
0         Non-trainable params
419       Total params
0.002     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 22:01:22,408][0m Trial 9 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 211 K 
-----------------------------------------
211 K     Trainable params
0         Non-trainable params
211 K     Total params
0.845     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 22:01:22,490][0m Trial 10 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 11.0 K
-----------------------------------------
11.0 K    Trainable params
0         Non-trainable params
11.0 K    Total params
0.044     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-07 22:38:37,614][0m Trial 11 finished with value: 0.06775060296058655 and parameters: {'num_of_naive_net_layers': 8, 'naive_net_output_dim_l0': 50, 'naive_net_output_dim_l1': 44, 'naive_net_output_dim_l2': 4, 'naive_net_output_dim_l3': 133, 'naive_net_output_dim_l4': 31, 'naive_net_output_dim_l5': 5, 'naive_net_output_dim_l6': 4, 'naive_net_output_dim_l7': 244}. Best is trial 11 with value: 0.06775060296058655.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 74.4 K
-----------------------------------------
74.4 K    Trainable params
0         Non-trainable params
74.4 K    Total params
0.297     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 22:38:37,694][0m Trial 12 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 23.3 K
-----------------------------------------
23.3 K    Trainable params
0         Non-trainable params
23.3 K    Total params
0.093     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 22:38:37,758][0m Trial 13 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 7.7 K 
-----------------------------------------
7.7 K     Trainable params
0         Non-trainable params
7.7 K     Total params
0.031     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 22:38:37,813][0m Trial 14 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 15.7 K
-----------------------------------------
15.7 K    Trainable params
0         Non-trainable params
15.7 K    Total params
0.063     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 22:38:37,877][0m Trial 15 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 37.7 K
-----------------------------------------
37.7 K    Trainable params
0         Non-trainable params
37.7 K    Total params
0.151     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-07 22:39:25,964][0m Trial 16 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 11.0 K
-----------------------------------------
11.0 K    Trainable params
0         Non-trainable params
11.0 K    Total params
0.044     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 22:39:26,027][0m Trial 17 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 11.1 K
-----------------------------------------
11.1 K    Trainable params
0         Non-trainable params
11.1 K    Total params
0.044     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 22:39:26,101][0m Trial 18 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 9.7 K 
-----------------------------------------
9.7 K     Trainable params
0         Non-trainable params
9.7 K     Total params
0.039     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 22:39:26,166][0m Trial 19 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 9.6 K 
-----------------------------------------
9.6 K     Trainable params
0         Non-trainable params
9.6 K     Total params
0.038     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 22:39:26,229][0m Trial 20 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 3.0 K 
-----------------------------------------
3.0 K     Trainable params
0         Non-trainable params
3.0 K     Total params
0.012     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 22:39:26,296][0m Trial 21 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 6.9 K 
-----------------------------------------
6.9 K     Trainable params
0         Non-trainable params
6.9 K     Total params
0.027     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 22:39:26,370][0m Trial 22 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 3.0 K 
-----------------------------------------
3.0 K     Trainable params
0         Non-trainable params
3.0 K     Total params
0.012     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 22:39:26,438][0m Trial 23 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 8.8 K 
-----------------------------------------
8.8 K     Trainable params
0         Non-trainable params
8.8 K     Total params
0.035     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 22:39:26,509][0m Trial 24 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 8.1 K 
-----------------------------------------
8.1 K     Trainable params
0         Non-trainable params
8.1 K     Total params
0.032     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 22:39:26,580][0m Trial 25 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 11.7 K
-----------------------------------------
11.7 K    Trainable params
0         Non-trainable params
11.7 K    Total params
0.047     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 22:39:26,653][0m Trial 26 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 37.0 K
-----------------------------------------
37.0 K    Trainable params
0         Non-trainable params
37.0 K    Total params
0.148     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-07 23:12:59,459][0m Trial 27 finished with value: 0.070663683116436 and parameters: {'num_of_naive_net_layers': 4, 'naive_net_output_dim_l0': 155, 'naive_net_output_dim_l1': 173, 'naive_net_output_dim_l2': 22, 'naive_net_output_dim_l3': 136}. Best is trial 11 with value: 0.06775060296058655.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 7.0 K 
-----------------------------------------
7.0 K     Trainable params
0         Non-trainable params
7.0 K     Total params
0.028     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 23:12:59,533][0m Trial 28 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 7.0 K 
-----------------------------------------
7.0 K     Trainable params
0         Non-trainable params
7.0 K     Total params
0.028     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-07 23:39:58,852][0m Trial 29 finished with value: 0.07157696038484573 and parameters: {'num_of_naive_net_layers': 3, 'naive_net_output_dim_l0': 150, 'naive_net_output_dim_l1': 30, 'naive_net_output_dim_l2': 12}. Best is trial 11 with value: 0.06775060296058655.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 8.2 K 
-----------------------------------------
8.2 K     Trainable params
0         Non-trainable params
8.2 K     Total params
0.033     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 23:39:58,918][0m Trial 30 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 37.6 K
-----------------------------------------
37.6 K    Trainable params
0         Non-trainable params
37.6 K    Total params
0.151     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 23:39:58,973][0m Trial 31 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 20.4 K
-----------------------------------------
20.4 K    Trainable params
0         Non-trainable params
20.4 K    Total params
0.082     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 23:39:59,022][0m Trial 32 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 17.8 K
-----------------------------------------
17.8 K    Trainable params
0         Non-trainable params
17.8 K    Total params
0.071     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 23:39:59,077][0m Trial 33 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 13.9 K
-----------------------------------------
13.9 K    Trainable params
0         Non-trainable params
13.9 K    Total params
0.056     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 23:39:59,138][0m Trial 34 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 99.4 K
-----------------------------------------
99.4 K    Trainable params
0         Non-trainable params
99.4 K    Total params
0.398     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 23:39:59,197][0m Trial 35 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 13.0 K
-----------------------------------------
13.0 K    Trainable params
0         Non-trainable params
13.0 K    Total params
0.052     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 23:39:59,262][0m Trial 36 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 22.3 K
-----------------------------------------
22.3 K    Trainable params
0         Non-trainable params
22.3 K    Total params
0.089     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-07 23:42:57,707][0m Trial 37 pruned. Trial was pruned at epoch 10.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 15.7 K
-----------------------------------------
15.7 K    Trainable params
0         Non-trainable params
15.7 K    Total params
0.063     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 23:42:57,769][0m Trial 38 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 16.4 K
-----------------------------------------
16.4 K    Trainable params
0         Non-trainable params
16.4 K    Total params
0.065     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 23:42:57,834][0m Trial 39 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 995   
-----------------------------------------
995       Trainable params
0         Non-trainable params
995       Total params
0.004     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-07 23:42:57,882][0m Trial 40 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 9.4 K 
-----------------------------------------
9.4 K     Trainable params
0         Non-trainable params
9.4 K     Total params
0.037     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-08 00:11:25,589][0m Trial 41 finished with value: 0.0659978911280632 and parameters: {'num_of_naive_net_layers': 3, 'naive_net_output_dim_l0': 196, 'naive_net_output_dim_l1': 32, 'naive_net_output_dim_l2': 12}. Best is trial 41 with value: 0.0659978911280632.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 8.8 K 
-----------------------------------------
8.8 K     Trainable params
0         Non-trainable params
8.8 K     Total params
0.035     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:11:25,644][0m Trial 42 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 4.1 K 
-----------------------------------------
4.1 K     Trainable params
0         Non-trainable params
4.1 K     Total params
0.016     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:11:25,700][0m Trial 43 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 15.5 K
-----------------------------------------
15.5 K    Trainable params
0         Non-trainable params
15.5 K    Total params
0.062     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-08 00:44:54,609][0m Trial 44 finished with value: 0.06746388971805573 and parameters: {'num_of_naive_net_layers': 5, 'naive_net_output_dim_l0': 209, 'naive_net_output_dim_l1': 49, 'naive_net_output_dim_l2': 31, 'naive_net_output_dim_l3': 16, 'naive_net_output_dim_l4': 10}. Best is trial 41 with value: 0.0659978911280632.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 32.4 K
-----------------------------------------
32.4 K    Trainable params
0         Non-trainable params
32.4 K    Total params
0.130     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:44:54,676][0m Trial 45 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 8.2 K 
-----------------------------------------
8.2 K     Trainable params
0         Non-trainable params
8.2 K     Total params
0.033     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:44:54,743][0m Trial 46 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 15.6 K
-----------------------------------------
15.6 K    Trainable params
0         Non-trainable params
15.6 K    Total params
0.062     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-08 00:46:01,162][0m Trial 47 pruned. Trial was pruned at epoch 2.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 2.0 K 
-----------------------------------------
2.0 K     Trainable params
0         Non-trainable params
2.0 K     Total params
0.008     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:01,215][0m Trial 48 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 7.6 K 
-----------------------------------------
7.6 K     Trainable params
0         Non-trainable params
7.6 K     Total params
0.030     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:01,273][0m Trial 49 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 5.2 K 
-----------------------------------------
5.2 K     Trainable params
0         Non-trainable params
5.2 K     Total params
0.021     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:01,330][0m Trial 50 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 7.3 K 
-----------------------------------------
7.3 K     Trainable params
0         Non-trainable params
7.3 K     Total params
0.029     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:01,382][0m Trial 51 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 6.0 K 
-----------------------------------------
6.0 K     Trainable params
0         Non-trainable params
6.0 K     Total params
0.024     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-08 00:46:48,737][0m Trial 52 pruned. Trial was pruned at epoch 2.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 3.7 K 
-----------------------------------------
3.7 K     Trainable params
0         Non-trainable params
3.7 K     Total params
0.015     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:48,796][0m Trial 53 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 19.4 K
-----------------------------------------
19.4 K    Trainable params
0         Non-trainable params
19.4 K    Total params
0.078     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:48,864][0m Trial 54 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 8.2 K 
-----------------------------------------
8.2 K     Trainable params
0         Non-trainable params
8.2 K     Total params
0.033     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:48,912][0m Trial 55 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 6.2 K 
-----------------------------------------
6.2 K     Trainable params
0         Non-trainable params
6.2 K     Total params
0.025     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:48,972][0m Trial 56 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 10.6 K
-----------------------------------------
10.6 K    Trainable params
0         Non-trainable params
10.6 K    Total params
0.042     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:49,051][0m Trial 57 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 110 K 
-----------------------------------------
110 K     Trainable params
0         Non-trainable params
110 K     Total params
0.442     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:49,126][0m Trial 58 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 29.6 K
-----------------------------------------
29.6 K    Trainable params
0         Non-trainable params
29.6 K    Total params
0.118     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:49,195][0m Trial 59 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 14.6 K
-----------------------------------------
14.6 K    Trainable params
0         Non-trainable params
14.6 K    Total params
0.058     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:49,266][0m Trial 60 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 6.3 K 
-----------------------------------------
6.3 K     Trainable params
0         Non-trainable params
6.3 K     Total params
0.025     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:49,323][0m Trial 61 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 8.4 K 
-----------------------------------------
8.4 K     Trainable params
0         Non-trainable params
8.4 K     Total params
0.033     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:49,374][0m Trial 62 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 5.0 K 
-----------------------------------------
5.0 K     Trainable params
0         Non-trainable params
5.0 K     Total params
0.020     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:49,434][0m Trial 63 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 7.4 K 
-----------------------------------------
7.4 K     Trainable params
0         Non-trainable params
7.4 K     Total params
0.030     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:49,486][0m Trial 64 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 12.4 K
-----------------------------------------
12.4 K    Trainable params
0         Non-trainable params
12.4 K    Total params
0.049     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:49,543][0m Trial 65 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 2.4 K 
-----------------------------------------
2.4 K     Trainable params
0         Non-trainable params
2.4 K     Total params
0.010     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:49,603][0m Trial 66 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 16.8 K
-----------------------------------------
16.8 K    Trainable params
0         Non-trainable params
16.8 K    Total params
0.067     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:49,674][0m Trial 67 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 7.4 K 
-----------------------------------------
7.4 K     Trainable params
0         Non-trainable params
7.4 K     Total params
0.030     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:49,742][0m Trial 68 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 3.5 K 
-----------------------------------------
3.5 K     Trainable params
0         Non-trainable params
3.5 K     Total params
0.014     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:49,805][0m Trial 69 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 71.5 K
-----------------------------------------
71.5 K    Trainable params
0         Non-trainable params
71.5 K    Total params
0.286     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:49,862][0m Trial 70 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 7.5 K 
-----------------------------------------
7.5 K     Trainable params
0         Non-trainable params
7.5 K     Total params
0.030     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:49,924][0m Trial 71 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 2.5 K 
-----------------------------------------
2.5 K     Trainable params
0         Non-trainable params
2.5 K     Total params
0.010     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:46:49,976][0m Trial 72 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 6.2 K 
-----------------------------------------
6.2 K     Trainable params
0         Non-trainable params
6.2 K     Total params
0.025     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-08 00:47:23,910][0m Trial 73 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 6.2 K 
-----------------------------------------
6.2 K     Trainable params
0         Non-trainable params
6.2 K     Total params
0.025     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:47:23,963][0m Trial 74 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 4.4 K 
-----------------------------------------
4.4 K     Trainable params
0         Non-trainable params
4.4 K     Total params
0.017     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:47:24,022][0m Trial 75 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 27.9 K
-----------------------------------------
27.9 K    Trainable params
0         Non-trainable params
27.9 K    Total params
0.111     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:47:24,089][0m Trial 76 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 38.4 K
-----------------------------------------
38.4 K    Trainable params
0         Non-trainable params
38.4 K    Total params
0.153     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:47:24,146][0m Trial 77 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 22.3 K
-----------------------------------------
22.3 K    Trainable params
0         Non-trainable params
22.3 K    Total params
0.089     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:47:24,212][0m Trial 78 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 3.7 K 
-----------------------------------------
3.7 K     Trainable params
0         Non-trainable params
3.7 K     Total params
0.015     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-08-08 00:48:17,922][0m Trial 79 pruned. Trial was pruned at epoch 2.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 1.5 K 
-----------------------------------------
1.5 K     Trainable params
0         Non-trainable params
1.5 K     Total params
0.006     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:17,981][0m Trial 80 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 11.1 K
-----------------------------------------
11.1 K    Trainable params
0         Non-trainable params
11.1 K    Total params
0.044     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:18,032][0m Trial 81 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 1.9 K 
-----------------------------------------
1.9 K     Trainable params
0         Non-trainable params
1.9 K     Total params
0.007     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:18,080][0m Trial 82 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 659   
-----------------------------------------
659       Trainable params
0         Non-trainable params
659       Total params
0.003     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:18,124][0m Trial 83 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 7.8 K 
-----------------------------------------
7.8 K     Trainable params
0         Non-trainable params
7.8 K     Total params
0.031     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:18,192][0m Trial 84 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 8.6 K 
-----------------------------------------
8.6 K     Trainable params
0         Non-trainable params
8.6 K     Total params
0.035     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:18,246][0m Trial 85 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 48.1 K
-----------------------------------------
48.1 K    Trainable params
0         Non-trainable params
48.1 K    Total params
0.193     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:18,302][0m Trial 86 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 29.9 K
-----------------------------------------
29.9 K    Trainable params
0         Non-trainable params
29.9 K    Total params
0.120     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:18,353][0m Trial 87 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 12.6 K
-----------------------------------------
12.6 K    Trainable params
0         Non-trainable params
12.6 K    Total params
0.051     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:18,416][0m Trial 88 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 31.6 K
-----------------------------------------
31.6 K    Trainable params
0         Non-trainable params
31.6 K    Total params
0.126     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:18,487][0m Trial 89 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 1.6 K 
-----------------------------------------
1.6 K     Trainable params
0         Non-trainable params
1.6 K     Total params
0.006     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:18,533][0m Trial 90 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 17.0 K
-----------------------------------------
17.0 K    Trainable params
0         Non-trainable params
17.0 K    Total params
0.068     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:18,594][0m Trial 91 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 9.1 K 
-----------------------------------------
9.1 K     Trainable params
0         Non-trainable params
9.1 K     Total params
0.036     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:18,663][0m Trial 92 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 14.6 K
-----------------------------------------
14.6 K    Trainable params
0         Non-trainable params
14.6 K    Total params
0.058     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:18,723][0m Trial 93 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 44.3 K
-----------------------------------------
44.3 K    Trainable params
0         Non-trainable params
44.3 K    Total params
0.177     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:18,803][0m Trial 94 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 13.5 K
-----------------------------------------
13.5 K    Trainable params
0         Non-trainable params
13.5 K    Total params
0.054     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:18,865][0m Trial 95 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 23.3 K
-----------------------------------------
23.3 K    Trainable params
0         Non-trainable params
23.3 K    Total params
0.093     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:18,924][0m Trial 96 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 3.8 K 
-----------------------------------------
3.8 K     Trainable params
0         Non-trainable params
3.8 K     Total params
0.015     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:18,976][0m Trial 97 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 12.1 K
-----------------------------------------
12.1 K    Trainable params
0         Non-trainable params
12.1 K    Total params
0.048     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:19,043][0m Trial 98 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-0f7bbfc0-e875-50de-b4b2-d28c11fba1ee]

  | Name      | Type       | Params
-----------------------------------------
0 | loss_fn   | MSELoss    | 0     
1 | criterion | MSELoss    | 0     
2 | layers    | Sequential | 18.9 K
-----------------------------------------
18.9 K    Trainable params
0         Non-trainable params
18.9 K    Total params
0.076     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-08-08 00:48:19,103][0m Trial 99 pruned. Trial was pruned at epoch 0.[0m
DeprecationWarning: 'source deactivate' is deprecated. Use 'conda deactivate'.
