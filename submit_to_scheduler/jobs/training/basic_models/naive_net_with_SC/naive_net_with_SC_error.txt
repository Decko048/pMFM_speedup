DeprecationWarning: 'source deactivate' is deprecated. Use 'conda deactivate'.
Global seed set to 5
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 30.2 K
3 | layers              | Sequential       | 80.9 K
---------------------------------------------------------
111 K     Trainable params
0         Non-trainable params
111 K     Total params
0.445     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-26 23:01:08,845][0m Trial 50 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 392 K 
3 | layers              | Sequential       | 13.5 K
---------------------------------------------------------
405 K     Trainable params
0         Non-trainable params
405 K     Total params
1.623     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 02:00:58,182][0m Trial 51 finished with value: 0.02730555087327957 and parameters: {'num_of_extract_SC_feat_layers': 2, 'extract_SC_feat_output_dim_l0': 171, 'extract_SC_feat_output_dim_l1': 13, 'num_of_naive_net_layers': 5, 'naive_net_output_dim_l0': 29, 'naive_net_output_dim_l1': 42, 'naive_net_output_dim_l2': 55, 'naive_net_output_dim_l3': 33, 'naive_net_output_dim_l4': 34}. Best is trial 0 with value: 0.024662066251039505.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 322 K 
3 | layers              | Sequential       | 18.9 K
---------------------------------------------------------
341 K     Trainable params
0         Non-trainable params
341 K     Total params
1.367     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 02:00:58,324][0m Trial 52 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 352 K 
3 | layers              | Sequential       | 19.8 K
---------------------------------------------------------
371 K     Trainable params
0         Non-trainable params
371 K     Total params
1.488     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 02:00:58,457][0m Trial 53 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 387 K 
3 | layers              | Sequential       | 14.1 K
---------------------------------------------------------
401 K     Trainable params
0         Non-trainable params
401 K     Total params
1.607     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 02:00:58,589][0m Trial 54 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 260 K 
3 | layers              | Sequential       | 20.2 K
---------------------------------------------------------
280 K     Trainable params
0         Non-trainable params
280 K     Total params
1.121     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 02:00:58,717][0m Trial 55 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 481 K 
3 | layers              | Sequential       | 53.4 K
---------------------------------------------------------
534 K     Trainable params
0         Non-trainable params
534 K     Total params
2.138     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 02:04:28,860][0m Trial 56 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 95.8 K
3 | layers              | Sequential       | 21.9 K
---------------------------------------------------------
117 K     Trainable params
0         Non-trainable params
117 K     Total params
0.471     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 05:01:57,923][0m Trial 57 finished with value: 0.02576626092195511 and parameters: {'num_of_extract_SC_feat_layers': 1, 'extract_SC_feat_output_dim_l0': 42, 'num_of_naive_net_layers': 5, 'naive_net_output_dim_l0': 53, 'naive_net_output_dim_l1': 50, 'naive_net_output_dim_l2': 77, 'naive_net_output_dim_l3': 19, 'naive_net_output_dim_l4': 8}. Best is trial 0 with value: 0.024662066251039505.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 91.2 K
3 | layers              | Sequential       | 14.6 K
---------------------------------------------------------
105 K     Trainable params
0         Non-trainable params
105 K     Total params
0.424     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 05:01:58,058][0m Trial 58 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 244 K 
3 | layers              | Sequential       | 11.0 K
---------------------------------------------------------
255 K     Trainable params
0         Non-trainable params
255 K     Total params
1.020     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 05:05:30,569][0m Trial 59 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 184 K 
3 | layers              | Sequential       | 22.8 K
---------------------------------------------------------
207 K     Trainable params
0         Non-trainable params
207 K     Total params
0.830     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 05:05:30,704][0m Trial 60 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 107 K 
3 | layers              | Sequential       | 10.6 K
---------------------------------------------------------
117 K     Trainable params
0         Non-trainable params
117 K     Total params
0.471     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 05:05:30,826][0m Trial 61 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 584 K 
3 | layers              | Sequential       | 5.5 K 
---------------------------------------------------------
590 K     Trainable params
0         Non-trainable params
590 K     Total params
2.360     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 05:05:30,957][0m Trial 62 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 422 K 
3 | layers              | Sequential       | 27.7 K
---------------------------------------------------------
450 K     Trainable params
0         Non-trainable params
450 K     Total params
1.802     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 05:05:31,079][0m Trial 63 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 16.0 K
3 | layers              | Sequential       | 64.8 K
---------------------------------------------------------
80.8 K    Trainable params
0         Non-trainable params
80.8 K    Total params
0.323     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 05:05:31,209][0m Trial 64 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 23.0 K
3 | layers              | Sequential       | 17.2 K
---------------------------------------------------------
40.2 K    Trainable params
0         Non-trainable params
40.2 K    Total params
0.161     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 05:09:10,199][0m Trial 65 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 289 K 
3 | layers              | Sequential       | 71.4 K
---------------------------------------------------------
361 K     Trainable params
0         Non-trainable params
361 K     Total params
1.445     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 05:09:10,346][0m Trial 66 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 155 K 
3 | layers              | Sequential       | 2.8 K 
---------------------------------------------------------
157 K     Trainable params
0         Non-trainable params
157 K     Total params
0.631     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 05:12:38,219][0m Trial 67 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 11.6 K
3 | layers              | Sequential       | 12.0 K
---------------------------------------------------------
23.7 K    Trainable params
0         Non-trainable params
23.7 K    Total params
0.095     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 05:16:11,919][0m Trial 68 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 37.3 K
3 | layers              | Sequential       | 8.4 K 
---------------------------------------------------------
45.7 K    Trainable params
0         Non-trainable params
45.7 K    Total params
0.183     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 05:16:12,064][0m Trial 69 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 9.1 K 
3 | layers              | Sequential       | 7.0 K 
---------------------------------------------------------
16.1 K    Trainable params
0         Non-trainable params
16.1 K    Total params
0.064     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 05:58:08,094][0m Trial 70 pruned. Trial was pruned at epoch 23.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 215 K 
3 | layers              | Sequential       | 11.3 K
---------------------------------------------------------
226 K     Trainable params
0         Non-trainable params
226 K     Total params
0.907     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 05:58:08,235][0m Trial 71 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 443 K 
3 | layers              | Sequential       | 14.5 K
---------------------------------------------------------
458 K     Trainable params
0         Non-trainable params
458 K     Total params
1.834     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 05:58:08,380][0m Trial 72 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 279 K 
3 | layers              | Sequential       | 5.7 K 
---------------------------------------------------------
284 K     Trainable params
0         Non-trainable params
284 K     Total params
1.139     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 05:58:08,516][0m Trial 73 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 50.5 K
3 | layers              | Sequential       | 16.0 K
---------------------------------------------------------
66.5 K    Trainable params
0         Non-trainable params
66.5 K    Total params
0.266     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 05:58:08,656][0m Trial 74 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 385 K 
3 | layers              | Sequential       | 7.5 K 
---------------------------------------------------------
393 K     Trainable params
0         Non-trainable params
393 K     Total params
1.573     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 05:58:08,787][0m Trial 75 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 77.6 K
3 | layers              | Sequential       | 23.7 K
---------------------------------------------------------
101 K     Trainable params
0         Non-trainable params
101 K     Total params
0.405     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 08:56:11,848][0m Trial 76 finished with value: 0.033895671367645264 and parameters: {'num_of_extract_SC_feat_layers': 1, 'extract_SC_feat_output_dim_l0': 34, 'num_of_naive_net_layers': 5, 'naive_net_output_dim_l0': 69, 'naive_net_output_dim_l1': 30, 'naive_net_output_dim_l2': 73, 'naive_net_output_dim_l3': 9, 'naive_net_output_dim_l4': 114}. Best is trial 0 with value: 0.024662066251039505.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 109 K 
3 | layers              | Sequential       | 69.0 K
---------------------------------------------------------
178 K     Trainable params
0         Non-trainable params
178 K     Total params
0.714     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 08:56:11,989][0m Trial 77 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 84.4 K
3 | layers              | Sequential       | 45.7 K
---------------------------------------------------------
130 K     Trainable params
0         Non-trainable params
130 K     Total params
0.520     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 08:56:12,119][0m Trial 78 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 59.3 K
3 | layers              | Sequential       | 45.7 K
---------------------------------------------------------
104 K     Trainable params
0         Non-trainable params
104 K     Total params
0.420     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 08:56:12,250][0m Trial 79 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 75.3 K
3 | layers              | Sequential       | 19.3 K
---------------------------------------------------------
94.6 K    Trainable params
0         Non-trainable params
94.6 K    Total params
0.378     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 08:56:12,378][0m Trial 80 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 142 K 
3 | layers              | Sequential       | 18.5 K
---------------------------------------------------------
160 K     Trainable params
0         Non-trainable params
160 K     Total params
0.644     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 08:56:12,518][0m Trial 81 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 16.0 K
3 | layers              | Sequential       | 20.0 K
---------------------------------------------------------
36.0 K    Trainable params
0         Non-trainable params
36.0 K    Total params
0.144     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 08:56:12,655][0m Trial 82 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 13.8 K
3 | layers              | Sequential       | 26.9 K
---------------------------------------------------------
40.6 K    Trainable params
0         Non-trainable params
40.6 K    Total params
0.162     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 08:56:12,789][0m Trial 83 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 47.4 K
3 | layers              | Sequential       | 20.7 K
---------------------------------------------------------
68.1 K    Trainable params
0         Non-trainable params
68.1 K    Total params
0.272     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 08:56:12,940][0m Trial 84 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 130 K 
3 | layers              | Sequential       | 20.3 K
---------------------------------------------------------
151 K     Trainable params
0         Non-trainable params
151 K     Total params
0.605     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 08:56:13,081][0m Trial 85 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 73.0 K
3 | layers              | Sequential       | 13.8 K
---------------------------------------------------------
86.7 K    Trainable params
0         Non-trainable params
86.7 K    Total params
0.347     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 08:56:13,210][0m Trial 86 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 350 K 
3 | layers              | Sequential       | 15.5 K
---------------------------------------------------------
365 K     Trainable params
0         Non-trainable params
365 K     Total params
1.464     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 08:59:45,625][0m Trial 87 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 22.7 K
3 | layers              | Sequential       | 69.9 K
---------------------------------------------------------
92.6 K    Trainable params
0         Non-trainable params
92.6 K    Total params
0.370     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 08:59:45,760][0m Trial 88 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 100 K 
3 | layers              | Sequential       | 36.3 K
---------------------------------------------------------
136 K     Trainable params
0         Non-trainable params
136 K     Total params
0.547     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 08:59:45,893][0m Trial 89 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 20.3 K
3 | layers              | Sequential       | 9.7 K 
---------------------------------------------------------
30.0 K    Trainable params
0         Non-trainable params
30.0 K    Total params
0.120     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 09:03:16,811][0m Trial 90 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 9.1 K 
3 | layers              | Sequential       | 6.5 K 
---------------------------------------------------------
15.6 K    Trainable params
0         Non-trainable params
15.6 K    Total params
0.063     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 09:03:16,944][0m Trial 91 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 9.1 K 
3 | layers              | Sequential       | 8.2 K 
---------------------------------------------------------
17.4 K    Trainable params
0         Non-trainable params
17.4 K    Total params
0.069     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 09:03:17,073][0m Trial 92 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 11.4 K
3 | layers              | Sequential       | 5.4 K 
---------------------------------------------------------
16.8 K    Trainable params
0         Non-trainable params
16.8 K    Total params
0.067     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 09:03:17,196][0m Trial 93 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 13.7 K
3 | layers              | Sequential       | 7.2 K 
---------------------------------------------------------
20.9 K    Trainable params
0         Non-trainable params
20.9 K    Total params
0.084     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 09:03:17,324][0m Trial 94 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 58.7 K
3 | layers              | Sequential       | 13.5 K
---------------------------------------------------------
72.2 K    Trainable params
0         Non-trainable params
72.2 K    Total params
0.289     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 09:03:17,463][0m Trial 95 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 79.8 K
3 | layers              | Sequential       | 6.1 K 
---------------------------------------------------------
85.9 K    Trainable params
0         Non-trainable params
85.9 K    Total params
0.344     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 09:03:17,585][0m Trial 96 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 219 K 
3 | layers              | Sequential       | 8.4 K 
---------------------------------------------------------
228 K     Trainable params
0         Non-trainable params
228 K     Total params
0.913     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 09:03:17,718][0m Trial 97 pruned. Trial was pruned at epoch 0.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 289 K 
3 | layers              | Sequential       | 27.9 K
---------------------------------------------------------
317 K     Trainable params
0         Non-trainable params
317 K     Total params
1.270     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/optuna/trial/_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.
  warnings.warn(
[32m[I 2022-07-27 09:06:51,114][0m Trial 98 pruned. Trial was pruned at epoch 1.[0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af8b64be-c2e4-bbfc-d4ff-388101a95196]

  | Name                | Type             | Params
---------------------------------------------------------
0 | loss_fn             | MSELoss          | 0     
1 | criterion           | MSELoss          | 0     
2 | extract_SC_feat_net | ExtractScFeatMLP | 415 K 
3 | layers              | Sequential       | 5.0 K 
---------------------------------------------------------
420 K     Trainable params
0         Non-trainable params
420 K     Total params
1.682     Total estimated model params size (MB)
/home/ftian/storage/miniconda/envs/pMFM_speedup-torch1.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[32m[I 2022-07-27 09:06:51,260][0m Trial 99 pruned. Trial was pruned at epoch 0.[0m
DeprecationWarning: 'source deactivate' is deprecated. Use 'conda deactivate'.
